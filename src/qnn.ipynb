{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.utils import algorithm_globals\n",
    "import wandb\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "from qiskit.algorithms.optimizers import COBYLA, SPSA\n",
    "from qiskit_ibm_runtime import Session, Sampler, Options\n",
    "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier  # noqa: E501\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.providers.fake_provider import FakeJakarta\n",
    "from qiskit_aer.noise import NoiseModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import shutil\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(test_labels, preds):\n",
    "    performance_metrics = {}\n",
    "\n",
    "    performance_metrics['accuracy'] = accuracy_score(test_labels, preds)\n",
    "    performance_metrics['precision'] = precision_score(test_labels, preds)\n",
    "    performance_metrics['recall'] = recall_score(test_labels, preds)\n",
    "    performance_metrics['f1_score'] = f1_score(test_labels, preds)\n",
    "\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "        output_dir,\n",
    "        output_filename,\n",
    "        downsample=True,\n",
    "        num_features=5,\n",
    "        scale_features=True,\n",
    "        pca_components=3):\n",
    "    \n",
    "    # set paths and filenames\n",
    "    base_data_dir = \"data\"\n",
    "    remote_file = \"https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/clinchem/66/9/10.1093_clinchem_hvaa134/1/hvaa134_supplementary_data.zip?Expires=1688683795&Signature=OFBeHiIqyKjgH8P3rv13K4P~8dbtJkimb2OEhxg0IRN5AUzLlGMdUQMaXP6d48b38m0Q1RXtyS7NmKOwQLF-f-nJtTjKs4DFqDUqWLCJh4SsSEbly1llz-7w6EkLddTtCSgV09nsrAs68Yz8u~vRW0PCwYuoGQFFg-Ob3e94xOULqq9Qf7Ut3N08Vmg1X6DgMqgQiVlWVieKPr50FcowFH987KxN7jqj~a0LvvAbyH6cZiiRupN517uJ4Qac-yScHNZN4~BaEBESvBryez-3GdKUwxl76TlRrOszxxIo6OUf5aHqj4T0IzYBAq08gVajya4yi-RGFM1uHRh3LGxt1g__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA\"\n",
    "\n",
    "    raw_dir = os.path.join(base_data_dir, \"raw\")\n",
    "    raw_data_file = os.path.join(raw_dir, \"amino_acid_data.csv\")\n",
    "    output_file = os.path.join(output_dir, output_filename + \".csv\")\n",
    "    output_metadata_file = os.path.join(output_dir, output_filename + \".json\")\n",
    "\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(raw_data_file):\n",
    "        urllib.request.urlretrieve(remote_file, os.path.join(raw_dir, \"data.zip\"))\n",
    "        with zipfile.ZipFile(os.path.join(raw_dir, \"data.zip\"), 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(raw_dir))\n",
    "        os.remove(os.path.join(raw_dir, \"data.zip\"))\n",
    "        \n",
    "        shutil.move(os.path.join(raw_dir, \"hvaa134-suppl_data\", \"clc317479-file001.csv\"), raw_data_file)\n",
    "        shutil.rmtree(os.path.join(raw_dir, \"hvaa134-suppl_data\"))\n",
    "\n",
    "\n",
    "    # read in data\n",
    "    df = pd.read_csv(raw_data_file)\n",
    "\n",
    "    # drop subject ID\n",
    "    df = df.drop(['SID'], axis = 1)\n",
    "\n",
    "    # convert SEX column to numerical codes\n",
    "    df.loc[(df['SEX'] == 'F'), 'SEX'] = 0\n",
    "    df.loc[(df['SEX'] == 'M'), 'SEX'] = 1\n",
    "    df.loc[(df['SEX'] == 'U'), 'SEX'] = 2\n",
    "\n",
    "    # convert ASA column to numerical codes\n",
    "    df.loc[(df['ASA'] == 'N'), 'ASA'] = 0\n",
    "    df.loc[(df['ASA'] == 'Y'), 'ASA'] = 1\n",
    "\n",
    "    # convert Allo column to numerical codes\n",
    "    df.loc[(df['Allo'] == 'N'), 'Allo'] = 0\n",
    "    df.loc[(df['Allo'] == 'Y'), 'Allo'] = 1\n",
    "\n",
    "    # convert Hcys column to numerical codes\n",
    "    df.loc[(df['Hcys'] == 'N'), 'Hcys'] = 0\n",
    "    df.loc[(df['Hcys'] == 'Y'), 'Hcys'] = 1\n",
    "\n",
    "    # convert labels from text to numerical codes \n",
    "    df.loc[(df['Class'] == 'No.significant.abnormality.detected.'), 'Class'] = 0\n",
    "    df.loc[(df['Class'] == 'X.Abnormal'), 'Class'] = 1\n",
    "\n",
    "    # downsample majority class\n",
    "    if downsample:\n",
    "        total_1 = sum(df[\"Class\"])\n",
    "        df_0 = df[df[\"Class\"]==0].sample(n = total_1)\n",
    "        # df_comb = df[df[\"Class\"]==1].append(df_0)\n",
    "        df_comb = pd.concat([df[df[\"Class\"]==1],df_0])\n",
    "        df = df_comb.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    percent_positive_class = df[df[\"Class\"]==1][\"Class\"].sum()/len(df[\"Class\"])\n",
    "\n",
    "    # check num_features\n",
    "    max_features = len(df.columns) - 1\n",
    "    if num_features > max_features:\n",
    "        raise Exception(f\"ERROR: For this dataset, the number of features must be {max_features} or less. Currently set to {num_features}\")\n",
    "    else:\n",
    "        # save data as X and labels as Y\n",
    "        X = df.iloc[:,:num_features].to_numpy()\n",
    "        Y = df.iloc[:,-1:].to_numpy().astype(\"int\").flatten()\n",
    "\n",
    "    # scale features\n",
    "    if scale_features:\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "    #reduce dimentionality of data\n",
    "    if pca_components is not None:\n",
    "        kernel_pca_rbf = KernelPCA(n_components=pca_components, kernel= \"rbf\")\n",
    "        kernel_pca_rbf.fit(X)\n",
    "        X = kernel_pca_rbf.transform(X)\n",
    "\n",
    "    # log final X shape\n",
    "    x_shape = [X.shape[0],X.shape[1]]\n",
    "\n",
    "    # write csv\n",
    "    data = np.concatenate((X, Y[:, np.newaxis].astype(int)), axis=1)\n",
    "    np.savetxt(output_file, data, delimiter=\",\")\n",
    "\n",
    "    metadata = {\n",
    "        \"downsample\": downsample,\n",
    "        \"num_features\": num_features,\n",
    "        \"scale_features\": scale_features,\n",
    "        \"records\": len(Y),\n",
    "        \"percent_positive_class\": percent_positive_class,\n",
    "        \"x_shape\": x_shape,\n",
    "        \"filename\": output_filename + \".csv\"\n",
    "    }\n",
    "\n",
    "    with open(output_metadata_file, \"w\") as outfile:\n",
    "        json.dump(metadata, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "        processed_dir,\n",
    "        downsample,\n",
    "        num_features,\n",
    "        scale_features,\n",
    "        pca_components):\n",
    "\n",
    "    processed_filename = f\"d{downsample}_f{num_features}_s{scale_features}\"\n",
    "    if pca_components is not None:\n",
    "        processed_filename += f\"_p{pca_components}\"\n",
    "\n",
    "    processed_file_path = os.path.join(\n",
    "        processed_dir,\n",
    "        processed_filename + \".csv\"\n",
    "    )\n",
    "    metadata_file_path = os.path.join(\n",
    "        processed_dir,\n",
    "        processed_filename + \".json\"\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(processed_file_path):\n",
    "        prepare_data(\n",
    "            output_dir=processed_dir,\n",
    "            output_filename=processed_filename,\n",
    "            downsample=downsample,\n",
    "            num_features=num_features,\n",
    "            scale_features=scale_features,\n",
    "            pca_components=pca_components\n",
    "        )\n",
    "\n",
    "    return (processed_filename, processed_file_path, metadata_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parity maps bitstrings to 0 or 1\n",
    "def parity(x):\n",
    "    return \"{:b}\".format(x).count(\"1\") % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and create local paths\n",
    "base_data_dir = \"data\"\n",
    "processed_dir = os.path.join(base_data_dir, \"processed\")\n",
    "result_dir = os.path.join(base_data_dir, \"results\")\n",
    "model_dir = os.path.join(base_data_dir, \"models\")\n",
    "\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Data\n",
    "filename, processed_file_path, metadata_file_path = load_data(\n",
    "    processed_dir=processed_dir,\n",
    "    downsample=True,\n",
    "    num_features=28,\n",
    "    scale_features=True,\n",
    "    pca_components=3\n",
    ")\n",
    "\n",
    "df = pd.read_csv(processed_file_path, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as X and labels as Y into numpy arrays\n",
    "X = df.iloc[:, :-1].to_numpy()\n",
    "Y = df.iloc[:, -1:].to_numpy().astype(\"int\").flatten()\n",
    "\n",
    "# create train/test datasets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(  # noqa: E501\n",
    "    X,\n",
    "    Y,\n",
    "    train_size=0.8,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_globals.random_seed = 42\n",
    "\n",
    "feature_map = ZZFeatureMap(\n",
    "    feature_dimension=train_features.shape[1],\n",
    "    reps=3\n",
    ")\n",
    "\n",
    "ansatz = RealAmplitudes(\n",
    "    num_qubits=train_features.shape[1],\n",
    "    reps=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct quantum circuit\n",
    "qc = QuantumCircuit(train_features.shape[1])\n",
    "qc.append(feature_map, range(train_features.shape[1]))\n",
    "qc.append(ansatz, range(train_features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.optimization_level = 3\n",
    "options.execution.shots = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = AerSimulator()\n",
    "service = QiskitRuntimeService()\n",
    "\n",
    "y_pred = None\n",
    "\n",
    "with Session(service, backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # corresponds to the number of classes,\n",
    "    # possible outcomes of the (parity) mapping\n",
    "    output_shape = 2\n",
    "\n",
    "    # construct QNN\n",
    "    sampler_qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters,\n",
    "        interpret=parity,\n",
    "        output_shape=output_shape,\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    optimizer = None\n",
    "    optimizer_config = wandb.config[\"quantum_params\"][\"optimizer\"]\n",
    "\n",
    "    optimizer = COBYLA(\n",
    "        maxiter=100\n",
    "    )\n",
    "\n",
    "    # construct classifier\n",
    "    sampler_classifier = NeuralNetworkClassifier(\n",
    "        neural_network=sampler_qnn,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "\n",
    "    sampler_classifier.fit(train_features, train_labels)\n",
    "\n",
    "    y_pred = sampler_classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_performance_metrics(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
